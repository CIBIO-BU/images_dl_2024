{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3703915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment optimizations\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128' if torch.cuda.is_available() else ''\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Distributed Animal Species Classification')\n",
    "    parser.add_argument('--data-dir', type=str, required=True, help='Path to dataset directory')\n",
    "    parser.add_argument('--epochs-per-subset', type=int, default=5, help='Epochs per subset')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, help='Batch size for training')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--min-samples', type=int, default=5000, help='Minimum samples per class')\n",
    "    parser.add_argument('--num-subsets', type=int, default=20, help='Number of data subsets')\n",
    "    parser.add_argument('--num-workers', type=int, default=1, help='Number of data loader workers')\n",
    "    parser.add_argument('--checkpoint-dir', type=str, default='checkpoints', help='Directory to save checkpoints')\n",
    "    parser.add_argument('--subset-range', type=str, default='all', help='Subset range to process (e.g., \"0-4\")')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_subsets(dataset, n_splits=10):\n",
    "    \"\"\"Split data into balanced subsets preserving class ratios\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    labels = [label for (_, label) in dataset.samples]\n",
    "    subsets = []\n",
    "    \n",
    "    for _, subset_indices in skf.split(dataset.samples, labels):\n",
    "        subsets.append(Subset(dataset, subset_indices))\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "def get_class_distribution(dataset):\n",
    "    \"\"\"Get class distribution as serializable dictionary\"\"\"\n",
    "    unique_classes, counts = torch.unique(\n",
    "        torch.tensor([s[1] for s in dataset.samples]), \n",
    "        return_counts=True\n",
    "    )\n",
    "    return {dataset.classes[int(k)]: int(v) for k, v in zip(unique_classes, counts)}\n",
    "\n",
    "def setup_model(num_classes, device):\n",
    "    \"\"\"Initialize and configure ResNet50 model\"\"\"\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    \n",
    "    # Freeze early layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'fc' not in name and 'layer4' not in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Enhanced classifier head\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1024, num_classes)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if device.type == 'cpu':\n",
    "        model = model.float()\n",
    "    return model\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Single training epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct = 0.0, 0\n",
    "    \n",
    "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, class_names, criterion, device):\n",
    "    \"\"\"Validation pass with per-class metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        target_names=class_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    val_loss /= len(loader.dataset)\n",
    "    val_acc = report['accuracy']\n",
    "    class_metrics = {\n",
    "        cls: {k: v for k, v in report[cls].items() \n",
    "              if k in ['precision', 'recall', 'f1-score']}\n",
    "        for cls in class_names\n",
    "    }\n",
    "    \n",
    "    return val_loss, val_acc, class_metrics\n",
    "\n",
    "def train_on_subsets(args, model, full_dataset, device):\n",
    "    \"\"\"Distributed training across subsets\"\"\"\n",
    "    subsets = create_subsets(full_dataset, args.num_subsets)\n",
    "    best_acc = 0.0\n",
    "    history = []\n",
    "    class_names = full_dataset.classes\n",
    "    \n",
    "    # Handle subset range\n",
    "    if args.subset_range == 'all':\n",
    "        subset_indices = range(args.num_subsets)\n",
    "    else:\n",
    "        start, end = map(int, args.subset_range.split('-'))\n",
    "        subset_indices = range(start, end+1)\n",
    "    \n",
    "    for i in subset_indices:\n",
    "        print(f\"\\nðŸŒ€ Processing subset {i+1}/{args.num_subsets}\")\n",
    "        \n",
    "        # Create train/val split for this subset\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            subsets[i].indices,\n",
    "            test_size=0.2,\n",
    "            stratify=[full_dataset.samples[idx][1] for idx in subsets[i].indices],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Apply transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        full_dataset.transform = transform\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            Subset(full_dataset, train_idx),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        # Validation uses center crop only\n",
    "        full_dataset.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            Subset(full_dataset, val_idx),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=min(2, args.num_workers)\n",
    "        )\n",
    "        \n",
    "        # Train on this subset\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(args.epochs_per_subset):\n",
    "            print(f\"\\nEpoch {epoch+1}/{args.epochs_per_subset} (Subset {i+1})\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, optimizer, nn.CrossEntropyLoss(), device\n",
    "            )\n",
    "            \n",
    "            val_loss, val_acc, class_metrics = validate(\n",
    "                model, val_loader, class_names, nn.CrossEntropyLoss(), device\n",
    "            )\n",
    "            \n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # Save history\n",
    "            history.append({\n",
    "                'subset': i,\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'class_metrics': class_metrics,\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'subset': i,\n",
    "                    'epoch': epoch,\n",
    "                    'val_acc': val_acc,\n",
    "                    'class_metrics': class_metrics,\n",
    "                    'args': vars(args)\n",
    "                }, os.path.join(args.checkpoint_dir, f'best_model_subset_{i}.pth'))\n",
    "                \n",
    "            # Print metrics\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2%}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%}\")\n",
    "            print(f\"Time: {epoch_time:.1f}s | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            \n",
    "            # Print top/bottom 3 classes by F1-score\n",
    "            sorted_classes = sorted(class_metrics.items(), key=lambda x: x[1]['f1-score'], reverse=True)\n",
    "            print(\"\\nTop 3 Classes:\")\n",
    "            for cls, metrics in sorted_classes[:3]:\n",
    "                print(f\"{cls}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1={metrics['f1-score']:.2f}\")\n",
    "            \n",
    "            print(\"\\nBottom 3 Classes:\")\n",
    "            for cls, metrics in sorted_classes[-3:]:\n",
    "                print(f\"{cls}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1={metrics['f1-score']:.2f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ðŸš€ Using device: {device}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"ðŸ“¦ Loading data...\")\n",
    "    full_dataset = datasets.ImageFolder(args.data_dir)\n",
    "    class_dist = get_class_distribution(full_dataset)\n",
    "    print(\"ðŸ“Š Class distribution:\", json.dumps(class_dist, indent=2))\n",
    "    \n",
    "    # Verify minimum samples\n",
    "    for cls, count in class_dist.items():\n",
    "        if count < args.min_samples:\n",
    "            raise ValueError(f\"Class {cls} has only {count} samples (minimum {args.min_samples} required)\")\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"ðŸ§  Initializing model...\")\n",
    "    model = setup_model(len(full_dataset.classes), device)\n",
    "    \n",
    "    # Distributed training\n",
    "    print(f\"ðŸ”¥ Starting distributed training on {args.num_subsets} subsets...\")\n",
    "    history = train_on_subsets(args, model, full_dataset, device)\n",
    "    \n",
    "    # Save final results\n",
    "    print(\"\\nðŸ† Training complete! Saving results...\")\n",
    "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Save history as JSON\n",
    "    with open(os.path.join(args.checkpoint_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    \n",
    "    # Generate and save plots\n",
    "    df = pd.DataFrame(history)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for subset in df['subset'].unique():\n",
    "        subset_data = df[df['subset'] == subset]\n",
    "        plt.plot(subset_data['epoch'], subset_data['val_acc'], label=f'Subset {subset+1}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Accuracy Across Subsets')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    last_epochs = df.groupby('subset').last()\n",
    "    class_metrics = pd.json_normalize(last_epochs['class_metrics'].explode().apply(pd.Series).stack())\n",
    "    class_metrics['class'] = class_metrics.index.get_level_values(1)\n",
    "    sns.boxplot(data=class_metrics, x='class', y='f1-score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Final F1-scores by Class')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(args.checkpoint_dir, 'training_metrics.png'))\n",
    "    print(f\"ðŸ“Š Saved metrics plot to {args.checkpoint_dir}/training_metrics.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

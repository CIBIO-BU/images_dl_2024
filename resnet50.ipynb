{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Environment optimizations\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128' if torch.cuda.is_available() else ''\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Progressive ResNet50 Wildlife Classification')\n",
    "    parser.add_argument('--data-dir', type=str, required=True, help='Path to dataset directory')\n",
    "    parser.add_argument('--epochs-per-subset', type=int, default=5, help='Epochs per subset')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, help='Batch size for training')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--min-samples', type=int, default=5000, help='Minimum samples per class')\n",
    "    parser.add_argument('--num-subsets', type=int, default=20, help='Number of data subsets')\n",
    "    parser.add_argument('--num-workers', type=int, default=0, help='Number of data loader workers')\n",
    "    parser.add_argument('--checkpoint-dir', type=str, default='checkpoints', help='Directory to save checkpoints')\n",
    "    parser.add_argument('--subset-range', type=str, default='all', help='Subset range to process (e.g., \"5-19\" for subsets 6-20)')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_subsets(dataset, n_splits=20):\n",
    "    \"\"\"Split data into balanced subsets preserving class ratios\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    labels = [label for (_, label) in dataset.samples]\n",
    "    subsets = []\n",
    "    \n",
    "    for _, subset_indices in skf.split(dataset.samples, labels):\n",
    "        subsets.append(Subset(dataset, subset_indices))\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "def get_class_distribution(dataset):\n",
    "    \"\"\"Get class distribution as serializable dictionary\"\"\"\n",
    "    unique_classes, counts = torch.unique(\n",
    "        torch.tensor([s[1] for s in dataset.samples]), \n",
    "        return_counts=True\n",
    "    )\n",
    "    return {dataset.classes[int(k)]: int(v) for k, v in zip(unique_classes, counts)}\n",
    "\n",
    "def setup_model(num_classes, device, subset_group=None):\n",
    "    \"\"\"Initialize and configure ResNet50 model with progressive unfreezing\"\"\"\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    \n",
    "    # Enhanced classifier head (always trainable)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1024, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Freeze all layers initially (will be selectively unfrozen)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc.requires_grad = True  # Always train classifier\n",
    "    \n",
    "    # Progressive unfreezing based on subset group\n",
    "    if subset_group is not None:\n",
    "        if subset_group >= 2:  # Subsets 6-10: Unfreeze layer4\n",
    "            for name, param in model.named_parameters():\n",
    "                if name.startswith('layer4'):\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        if subset_group >= 3:  # Subsets 11-15: Unfreeze layer3 and layer4\n",
    "            for name, param in model.named_parameters():\n",
    "                if name.startswith('layer3') or name.startswith('layer4'):\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        if subset_group >= 4:  # Subsets 16-20: Unfreeze all layers\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if device.type == 'cpu':\n",
    "        model = model.float()\n",
    "    return model\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Single training epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct = 0.0, 0\n",
    "    \n",
    "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, class_names, criterion, device):\n",
    "    \"\"\"Validation pass with per-class metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        target_names=class_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    val_loss /= len(loader.dataset)\n",
    "    val_acc = report['accuracy']\n",
    "    class_metrics = {\n",
    "        cls: {k: v for k, v in report[cls].items() \n",
    "              if k in ['precision', 'recall', 'f1-score']}\n",
    "        for cls in class_names\n",
    "    }\n",
    "    \n",
    "    return val_loss, val_acc, class_metrics\n",
    "\n",
    "def train_on_subsets(args, model, full_dataset, device):\n",
    "    \"\"\"Distributed training across subsets with progressive unfreezing\"\"\"\n",
    "    subsets = create_subsets(full_dataset, args.num_subsets)\n",
    "    best_acc = 0.0\n",
    "    history = []\n",
    "    class_names = full_dataset.classes\n",
    "    \n",
    "    # Handle subset range\n",
    "    if args.subset_range == 'all':\n",
    "        subset_indices = range(args.num_subsets)\n",
    "    else:\n",
    "        start, end = map(int, args.subset_range.split('-'))\n",
    "        subset_indices = range(start, end+1)\n",
    "    \n",
    "    # Load checkpoint if resuming\n",
    "    resume_subset = None\n",
    "    if subset_indices[0] > 0:\n",
    "        checkpoint_path = os.path.join(args.checkpoint_dir, f'best_model_subset_{subset_indices[0]-1}.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"âœ… Loaded checkpoint from subset {subset_indices[0]-1}\")\n",
    "            resume_subset = subset_indices[0]\n",
    "    \n",
    "    for i in subset_indices:\n",
    "        print(f\"\\nðŸŒ€ Processing subset {i+1}/{args.num_subsets}\")\n",
    "        \n",
    "        # Calculate subset group (1-4)\n",
    "        subset_group = (i // 5) + 1\n",
    "        \n",
    "        # Reconfigure model for current subset group\n",
    "        if resume_subset and i >= resume_subset:\n",
    "            model = setup_model(len(class_names), device, subset_group)\n",
    "            model.load_state_dict(torch.load(\n",
    "                os.path.join(args.checkpoint_dir, f'best_model_subset_{i-1}.pth')\n",
    "            )['model_state_dict'])\n",
    "            print(f\"ðŸ”“ Unfreezing strategy for subset group {subset_group}\")\n",
    "        \n",
    "        # Create train/val split\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            subsets[i].indices,\n",
    "            test_size=0.2,\n",
    "            stratify=[full_dataset.samples[idx][1] for idx in subsets[i].indices],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Debug check for overlap\n",
    "        overlap = set(train_idx) & set(val_idx)\n",
    "        assert len(overlap) == 0, f\"Data leakage detected in subset {i}!\"\n",
    "        \n",
    "        # Training transforms\n",
    "        full_dataset.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            Subset(full_dataset, train_idx),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        # Validation transforms\n",
    "        full_dataset.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            Subset(full_dataset, val_idx),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=min(2, args.num_workers)\n",
    "        )\n",
    "        \n",
    "        # Configure optimizer\n",
    "        trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = optim.AdamW(trainable_params, lr=args.lr, weight_decay=1e-4)\n",
    "        \n",
    "        # Resume optimizer state if continuing same subset group\n",
    "        if resume_subset and i == resume_subset:\n",
    "            try:\n",
    "                optimizer.load_state_dict(torch.load(\n",
    "                    os.path.join(args.checkpoint_dir, f'best_model_subset_{i-1}.pth'),\n",
    "                    weights_only=True\n",
    "                )['optimizer_state_dict'])\n",
    "                print(\"ðŸ”„ Resuming optimizer state\")\n",
    "            except:\n",
    "                print(\"âš ï¸ Could not resume optimizer state, starting fresh\")\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(args.epochs_per_subset):\n",
    "            print(f\"\\nEpoch {epoch+1}/{args.epochs_per_subset} (Subset {i+1}, Group {subset_group})\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, optimizer, nn.CrossEntropyLoss(), device\n",
    "            )\n",
    "            \n",
    "            val_loss, val_acc, class_metrics = validate(\n",
    "                model, val_loader, class_names, nn.CrossEntropyLoss(), device\n",
    "            )\n",
    "            \n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # Save history\n",
    "            history.append({\n",
    "                'subset': i,\n",
    "                'subset_group': subset_group,\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'class_metrics': class_metrics,\n",
    "                'lr': optimizer.param_groups[0]['lr'],\n",
    "                'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'subset': i,\n",
    "                    'epoch': epoch,\n",
    "                    'val_acc': val_acc,\n",
    "                    'class_metrics': class_metrics,\n",
    "                    'args': vars(args)\n",
    "                }, os.path.join(args.checkpoint_dir, f'best_model_subset_{i}.pth'))\n",
    "                \n",
    "            # Print metrics\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2%}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%}\")\n",
    "            print(f\"Time: {epoch_time:.1f}s | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            print(f\"Trainable params: {history[-1]['trainable_params']:,}\")\n",
    "            \n",
    "            # Log top/bottom classes\n",
    "            sorted_classes = sorted(class_metrics.items(), key=lambda x: x[1]['f1-score'], reverse=True)\n",
    "            print(\"\\nTop 3 Classes:\")\n",
    "            for cls, metrics in sorted_classes[:3]:\n",
    "                print(f\"{cls}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1={metrics['f1-score']:.2f}\")\n",
    "            \n",
    "            print(\"\\nBottom 3 Classes:\")\n",
    "            for cls, metrics in sorted_classes[-3:]:\n",
    "                print(f\"{cls}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1={metrics['f1-score']:.2f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ðŸš€ Using device: {device}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"ðŸ“¦ Loading data...\")\n",
    "    full_dataset = datasets.ImageFolder(args.data_dir)\n",
    "    class_dist = get_class_distribution(full_dataset)\n",
    "    print(\"ðŸ“Š Class distribution:\", json.dumps(class_dist, indent=2))\n",
    "    \n",
    "    # Verify minimum samples\n",
    "    for cls, count in class_dist.items():\n",
    "        if count < args.min_samples:\n",
    "            raise ValueError(f\"Class {cls} has only {count} samples (minimum {args.min_samples} required)\")\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"ðŸ§  Initializing model...\")\n",
    "    initial_subset_group = (int(args.subset_range.split('-')[0]) // 5) + 1 if args.subset_range != 'all' else 1\n",
    "    model = setup_model(len(full_dataset.classes), device, initial_subset_group)\n",
    "    \n",
    "    # Training\n",
    "    print(f\"ðŸ”¥ Starting training on subsets {args.subset_range}...\")\n",
    "    history = train_on_subsets(args, model, full_dataset, device)\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nðŸ† Training complete! Saving results...\")\n",
    "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(args.checkpoint_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    \n",
    "    # Generate plots\n",
    "    df = pd.DataFrame(history)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for subset in df['subset'].unique():\n",
    "        subset_data = df[df['subset'] == subset]\n",
    "        plt.plot(subset_data['epoch'], subset_data['val_acc'], label=f'Subset {subset+1}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Accuracy Across Subsets')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    last_epochs = df.groupby('subset').last()\n",
    "    class_metrics = pd.json_normalize(last_epochs['class_metrics'].explode().apply(pd.Series).stack())\n",
    "    class_metrics['class'] = class_metrics.index.get_level_values(1)\n",
    "    sns.boxplot(data=class_metrics, x='class', y='f1-score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Final F1-scores by Class')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(args.checkpoint_dir, 'training_metrics.png'))\n",
    "    print(f\"ðŸ“Š Saved metrics plot to {args.checkpoint_dir}/training_metrics.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from PIL import ImageFile\n",
    "\n",
    "# Environment optimizations\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128' if torch.cuda.is_available() else ''\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Handle corrupted images\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Progressive ResNet50 Wildlife Classification')\n",
    "    parser.add_argument('--data-dir', type=str, default='lila_species_cropped', help='Path to dataset directory')\n",
    "    parser.add_argument('--epochs-per-subset', type=int, default=5, help='Epochs per subset')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, help='Batch size for training')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate')\n",
    "    parser.add_argument('--min-samples', type=int, default=4000, help='Minimum samples per class')\n",
    "    parser.add_argument('--num-subsets', type=int, default=20, help='Number of data subsets')\n",
    "    parser.add_argument('--num-workers', type=int, default=1, help='Number of data loader workers')\n",
    "    parser.add_argument('--checkpoint-dir', type=str, default='checkpoints', help='Directory to save checkpoints')\n",
    "    parser.add_argument('--subset-range', type=str, default='0-19', help='Subset range to process (e.g., \"5-19\")')\n",
    "    parser.add_argument('--early-stopping', type=int, default=3, help='Patience for early stopping')\n",
    "    parser.add_argument('--weight-decay', type=float, default=1e-4, help='Weight decay for optimizer')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate')\n",
    "    parser.add_argument('--mixup-alpha', type=float, default=0.2, help='Alpha for mixup augmentation')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_subsets(dataset, n_splits=20):\n",
    "    \"\"\"Split data into balanced subsets preserving class ratios\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    labels = [label for (_, label) in dataset.samples]\n",
    "    subsets = []\n",
    "    \n",
    "    for _, subset_indices in skf.split(np.zeros(len(labels)), labels):  # Dummy X\n",
    "        subsets.append(Subset(dataset, subset_indices))\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "def get_class_distribution(dataset):\n",
    "    \"\"\"Get class distribution as serializable dictionary\"\"\"\n",
    "    if isinstance(dataset, Subset):\n",
    "        # For Subset objects, access through the original dataset\n",
    "        labels = [dataset.dataset.samples[i][1] for i in dataset.indices]\n",
    "        unique_classes, counts = torch.unique(torch.tensor(labels), return_counts=True)\n",
    "        return {dataset.dataset.classes[int(k)]: int(v) for k, v in zip(unique_classes, counts)}\n",
    "    elif hasattr(dataset, 'samples'):\n",
    "        # For regular ImageFolder datasets\n",
    "        labels = [s[1] for s in dataset.samples]\n",
    "        unique_classes, counts = torch.unique(torch.tensor(labels), return_counts=True)\n",
    "        return {dataset.classes[int(k)]: int(v) for k, v in zip(unique_classes, counts)}\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type\")\n",
    "\n",
    "def setup_model(num_classes, device, subset_group=None):\n",
    "    \"\"\"Initialize and configure ResNet50 model with progressive unfreezing\"\"\"\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    \n",
    "    # Enhanced classifier head with configurable dropout\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1024, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Always keep classifier trainable\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Progressive unfreezing based on subset group\n",
    "    if subset_group is None:\n",
    "        # Default: train only classifier\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.fc.requires_grad = True\n",
    "    else:\n",
    "        # Group 1: Only classifier (already set)\n",
    "        if subset_group >= 2:  # Group 2: Unfreeze layer4\n",
    "            for param in model.layer4.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        if subset_group >= 3:  # Group 3: Unfreeze layer3 and layer4\n",
    "            for param in model.layer3.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        if subset_group >= 4:  # Group 4: Unfreeze all layers\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if device.type == 'cpu':\n",
    "        model = model.float()\n",
    "    \n",
    "    # Debug: Verify trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"ðŸ”§ Model setup: Group {subset_group}, Trainable params: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device, mixup_alpha=0.0):\n",
    "    \"\"\"Single training epoch with optional mixup augmentation\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct = 0.0, 0\n",
    "    \n",
    "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply mixup if enabled\n",
    "        if mixup_alpha > 0:\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, mixup_alpha, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if mixup_alpha > 0:\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # For mixup, we calculate accuracy differently\n",
    "        if mixup_alpha > 0:\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (lam * preds.eq(targets_a).sum().item() + \n",
    "                       (1 - lam) * preds.eq(targets_b).sum().item())\n",
    "        else:\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, class_names, criterion, device):\n",
    "    \"\"\"Validation pass with per-class metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Generate comprehensive classification report\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        target_names=class_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    val_loss /= len(loader.dataset)\n",
    "    val_acc = report['accuracy']\n",
    "    class_metrics = {\n",
    "        cls: {k: v for k, v in report[cls].items() \n",
    "              if k in ['precision', 'recall', 'f1-score']}\n",
    "        for cls in class_names\n",
    "    }\n",
    "    \n",
    "    return val_loss, val_acc, class_metrics, cm_normalized\n",
    "\n",
    "def get_weighted_sampler(dataset):\n",
    "    \"\"\"Create weighted sampler for imbalanced classes\"\"\"\n",
    "    # Handle both Dataset and Subset objects\n",
    "    if isinstance(dataset, Subset):\n",
    "        class_counts = get_class_distribution(dataset)\n",
    "        class_weights = {cls: 1./count for cls, count in class_counts.items()}\n",
    "        sample_weights = [class_weights[dataset.dataset.classes[label]] \n",
    "                         for _, label in [dataset.dataset.samples[i] for i in dataset.indices]]\n",
    "    else:\n",
    "        class_counts = get_class_distribution(dataset)\n",
    "        class_weights = {cls: 1./count for cls, count in class_counts.items()}\n",
    "        sample_weights = [class_weights[dataset.classes[label]] \n",
    "                         for _, label in dataset.samples]\n",
    "    \n",
    "    return WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "def train_on_subsets(args, model, full_dataset, device):\n",
    "    \"\"\"Distributed training across subsets with progressive unfreezing\"\"\"\n",
    "    subsets = create_subsets(full_dataset, args.num_subsets)\n",
    "    best_acc = 0.0\n",
    "    history = []\n",
    "    class_names = full_dataset.classes\n",
    "    \n",
    "    # Handle subset range\n",
    "    start, end = map(int, args.subset_range.split('-'))\n",
    "    subset_indices = range(start, end+1)\n",
    "    \n",
    "    # Early stopping tracking\n",
    "    early_stopping_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Load checkpoint if resuming\n",
    "    resume_subset = None\n",
    "    if start > 0:\n",
    "        checkpoint_path = os.path.join(args.checkpoint_dir, f'best_model_subset_{start-1}.pth')\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"âœ… Loaded checkpoint from subset {start-1}\")\n",
    "            best_acc = checkpoint.get('val_acc', 0.0)\n",
    "            resume_subset = start\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ No checkpoint found at {checkpoint_path}, starting fresh\")\n",
    "            resume_subset = None\n",
    "    \n",
    "    for i in subset_indices:\n",
    "        print(f\"\\nðŸŒ€ Processing subset {i+1}/{args.num_subsets}\")\n",
    "        \n",
    "        # Calculate subset group (1-4)\n",
    "        subset_group = (i // 5) + 1\n",
    "        prev_group = ((i-1) // 5) + 1 if i > 0 else 1\n",
    "        \n",
    "        # Ensure checkpoint directory exists\n",
    "        os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        # Handle group transitions\n",
    "        if subset_group != prev_group:\n",
    "            print(f\"\\nðŸš€ GROUP TRANSITION DETECTED: Moving from group {prev_group} to {subset_group}\")\n",
    "            model = setup_model(len(class_names), device, subset_group)\n",
    "            \n",
    "            # Try to load compatible parameters from previous checkpoint\n",
    "            prev_checkpoint_path = os.path.join(args.checkpoint_dir, f'best_model_subset_{i-1}.pth')\n",
    "            if os.path.exists(prev_checkpoint_path):\n",
    "                try:\n",
    "                    prev_checkpoint = torch.load(prev_checkpoint_path, map_location=device)\n",
    "                    current_state = model.state_dict()\n",
    "                    \n",
    "                    # Load only matching parameters\n",
    "                    matched_params = {\n",
    "                        k: v for k, v in prev_checkpoint['model_state_dict'].items()\n",
    "                        if k in current_state and v.shape == current_state[k].shape\n",
    "                    }\n",
    "                    \n",
    "                    current_state.update(matched_params)\n",
    "                    model.load_state_dict(current_state)\n",
    "                    print(f\"ðŸ”„ Loaded {len(matched_params)}/{len(current_state)} compatible parameters across group transition\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Group transition load failed: {str(e)}\")\n",
    "                    print(\"ðŸ” Starting fresh for new group\")\n",
    "            else:\n",
    "                print(\"âš ï¸ No previous checkpoint found for group transition\")\n",
    "\n",
    "        # Create train/val split\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            subsets[i].indices,\n",
    "            test_size=0.2,\n",
    "            stratify=[full_dataset.samples[idx][1] for idx in subsets[i].indices],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Debug check for overlap\n",
    "        overlap = set(train_idx) & set(val_idx)\n",
    "        assert len(overlap) == 0, f\"Data leakage detected in subset {i}!\"\n",
    "\n",
    "        # Training transforms with augmentation\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Apply transforms to subset\n",
    "        full_dataset.transform = train_transform\n",
    "        train_subset = Subset(full_dataset, train_idx)\n",
    "        \n",
    "        # Create weighted sampler for imbalanced classes\n",
    "        sampler = get_weighted_sampler(Subset(full_dataset, train_idx))\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=args.batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=args.num_workers > 0\n",
    "        )\n",
    "        \n",
    "        # Validation transforms\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        full_dataset.transform = val_transform\n",
    "        val_loader = DataLoader(\n",
    "            Subset(full_dataset, val_idx),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=min(1, args.num_workers),\n",
    "            persistent_workers=args.num_workers > 0\n",
    "        )\n",
    "        \n",
    "        # Configure optimizer with weight decay\n",
    "        trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        optimizer = optim.AdamW(trainable_params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "        # Resume optimizer state if continuing same subset group\n",
    "        if i > start and subset_group == prev_group:\n",
    "            try:\n",
    "                checkpoint_path = os.path.join(args.checkpoint_dir, f'best_model_subset_{i-1}.pth')\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "                    if 'optimizer_state_dict' in checkpoint:\n",
    "                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                        print(\"ðŸ”„ Resuming optimizer state\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Optimizer state not loaded: {str(e)}\")\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2\n",
    "        )\n",
    "        \n",
    "        # Early stopping tracking per subset\n",
    "        subset_best_acc = 0.0\n",
    "        subset_early_stop = 0\n",
    "        \n",
    "        for epoch in range(args.epochs_per_subset):\n",
    "            print(f\"\\nEpoch {epoch+1}/{args.epochs_per_subset} (Subset {i+1}, Group {subset_group})\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, optimizer, nn.CrossEntropyLoss(), device, args.mixup_alpha\n",
    "            )\n",
    "            \n",
    "            val_loss, val_acc, class_metrics, cm = validate(\n",
    "                model, val_loader, class_names, nn.CrossEntropyLoss(), device\n",
    "            )\n",
    "            \n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # Save history with additional metrics\n",
    "            history.append({\n",
    "                'subset': i,\n",
    "                'subset_group': subset_group,\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'class_metrics': class_metrics,\n",
    "                'lr': optimizer.param_groups[0]['lr'],\n",
    "                'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "                'confusion_matrix': cm.tolist()  # Save normalized confusion matrix\n",
    "            })\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_acc > subset_best_acc:\n",
    "                subset_best_acc = val_acc\n",
    "                subset_early_stop = 0\n",
    "                best_model_state = model.state_dict()\n",
    "            else:\n",
    "                subset_early_stop += 1\n",
    "                if subset_early_stop >= args.early_stopping:\n",
    "                    print(f\"ðŸ›‘ Early stopping triggered for subset {i} after {epoch+1} epochs\")\n",
    "                    break\n",
    "            \n",
    "            # Enhanced checkpoint saving\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                checkpoint_path = os.path.join(args.checkpoint_dir, f'best_model_subset_{i}.pth')\n",
    "                temp_path = f\"{checkpoint_path}.tmp\"\n",
    "                \n",
    "                try:\n",
    "                    checkpoint = {\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'subset': i,\n",
    "                        'epoch': epoch,\n",
    "                        'val_acc': val_acc,\n",
    "                        'class_metrics': class_metrics,\n",
    "                        'args': vars(args),\n",
    "                        'group': subset_group,\n",
    "                        'class_names': class_names\n",
    "                    }\n",
    "                    \n",
    "                    torch.save(checkpoint, temp_path)\n",
    "                    os.replace(temp_path, checkpoint_path)\n",
    "                    print(f\"ðŸ’¾ Saved checkpoint for subset {i} (Group {subset_group})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"ðŸ”¥ ERROR SAVING CHECKPOINT: {str(e)}\")\n",
    "                    emergency_path = f\"/tmp/emergency_subset_{i}.pth\"\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'group': subset_group,\n",
    "                        'epoch': epoch,\n",
    "                        'val_acc': val_acc\n",
    "                    }, emergency_path)\n",
    "                    print(f\"ðŸš¨ EMERGENCY BACKUP SAVED TO: {emergency_path}\")\n",
    "            \n",
    "            # Print metrics\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2%}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%}\")\n",
    "            print(f\"Time: {epoch_time:.1f}s | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            print(f\"Trainable params: {history[-1]['trainable_params']:,}\")\n",
    "            \n",
    "            # Log top/bottom classes\n",
    "            sorted_classes = sorted(class_metrics.items(), key=lambda x: x[1]['f1-score'], reverse=True)\n",
    "            print(\"\\nTop 3 Classes:\")\n",
    "            for cls, metrics in sorted_classes[:3]:\n",
    "                print(f\"{cls}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1={metrics['f1-score']:.2f}\")\n",
    "            \n",
    "            print(\"\\nBottom 3 Classes:\")\n",
    "            for cls, metrics in sorted_classes[-3:]:\n",
    "                print(f\"{cls}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1={metrics['f1-score']:.2f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def save_plots(history, class_names, checkpoint_dir):\n",
    "    \"\"\"Save comprehensive training plots and metrics\"\"\"\n",
    "    df = pd.DataFrame(history)\n",
    "    \n",
    "    # Create plot directory\n",
    "    plot_dir = os.path.join(checkpoint_dir, 'plots')\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Training and Validation Metrics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for subset in df['subset'].unique():\n",
    "        subset_data = df[df['subset'] == subset]\n",
    "        plt.plot(subset_data['epoch'], subset_data['val_acc'], label=f'Subset {subset+1}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Accuracy Across Subsets')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for subset in df['subset'].unique():\n",
    "        subset_data = df[df['subset'] == subset]\n",
    "        plt.plot(subset_data['epoch'], subset_data['val_loss'], label=f'Subset {subset+1}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Loss Across Subsets')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Learning rate plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for subset in df['subset'].unique():\n",
    "        subset_data = df[df['subset'] == subset]\n",
    "        plt.plot(subset_data['epoch'], subset_data['lr'], label=f'Subset {subset+1}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # F1-scores by class\n",
    "    plt.subplot(2, 2, 4)\n",
    "    all_class_metrics = []\n",
    "    for _, row in df.iterrows():\n",
    "        for class_name, metrics in row['class_metrics'].items():\n",
    "            metrics['class'] = class_name\n",
    "            metrics['subset'] = row['subset']\n",
    "            metrics['epoch'] = row['epoch']\n",
    "            all_class_metrics.append(metrics)\n",
    "    \n",
    "    class_df = pd.DataFrame(all_class_metrics)\n",
    "    final_metrics = class_df.groupby(['class', 'subset']).last().reset_index()\n",
    "    \n",
    "    sns.boxplot(data=final_metrics, x='class', y='f1-score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Final F1-scores by Class')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'training_metrics.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Confusion Matrix from last epoch\n",
    "    last_confusion = history[-1]['confusion_matrix']\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(last_confusion, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(plot_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Class-wise metric trends\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i, metric in enumerate(['precision', 'recall', 'f1-score']):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        for cls in class_names[:10]:  # Plot first 10 classes for clarity\n",
    "            cls_data = class_df[class_df['class'] == cls]\n",
    "            plt.plot(cls_data['epoch'], cls_data[metric], label=cls)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f'{metric.capitalize()} Trends')\n",
    "        if i == 2:\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'class_metrics.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ðŸš€ Using device: {device}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"ðŸ“¦ Loading data...\")\n",
    "    full_dataset = datasets.ImageFolder(args.data_dir)\n",
    "    class_dist = get_class_distribution(full_dataset)\n",
    "    print(\"ðŸ“Š Class distribution:\", json.dumps(class_dist, indent=2))\n",
    "    \n",
    "    # Verify minimum samples\n",
    "    for cls, count in class_dist.items():\n",
    "        if count < args.min_samples:\n",
    "            raise ValueError(f\"Class {cls} has only {count} samples (minimum {args.min_samples} required)\")\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"ðŸ§  Initializing model...\")\n",
    "    initial_subset_group = (int(args.subset_range.split('-')[0]) // 5) + 1\n",
    "    model = setup_model(len(full_dataset.classes), device, initial_subset_group)\n",
    "    \n",
    "    # Verify model has trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    if trainable_params == 0:\n",
    "        raise RuntimeError(\"Model initialized with no trainable parameters! Check setup_model().\")\n",
    "    print(f\"âœ… Model initialized with {trainable_params:,} trainable parameters\")\n",
    "    \n",
    "    # Training\n",
    "    print(f\"ðŸ”¥ Starting training on subsets {args.subset_range}...\")\n",
    "    history = train_on_subsets(args, model, full_dataset, device)\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nðŸ† Training complete! Saving results...\")\n",
    "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Save full history\n",
    "    with open(os.path.join(args.checkpoint_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    \n",
    "    # Generate and save plots\n",
    "    save_plots(history, full_dataset.classes, args.checkpoint_dir)\n",
    "    print(f\"ðŸ“Š Saved training plots to {args.checkpoint_dir}/plots/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil -m cp -r \"gs://public-datasets-lila/wcs-unzipped/*\" ./wcs_dataset_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 - <<EOF\n",
    "import ijson\n",
    "from collections import defaultdict\n",
    "\n",
    "# Count category occurrences from local file\n",
    "category_counts = defaultdict(int)\n",
    "with open(\"wcs_camera_traps.json\", \"rb\") as f:  # 'rb' mode for ijson\n",
    "    for ann in ijson.items(f, \"annotations.item\"):\n",
    "        category_counts[ann[\"category_id\"]] += 1\n",
    "        print(f\"{ann['category_name']}:{category_counts[ann['category_id']]}\")\n",
    "\n",
    "# Get top 20 categories\n",
    "top_50 = sorted(category_counts.items(), key=lambda x: -x[1])[:50]\n",
    "top_50_ids = [cat_id for cat_id, _ in top_50]\n",
    "\n",
    "print(\"Top 20 category IDs:\", top_50_ids)\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load annotation data\n",
    "with open(\"wcs_camera_traps.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define target category IDs\n",
    "target_category_ids = {\n",
    "    2, 372, 71, 96, 111, 374, 3, 115, 10,\n",
    "    317, 90, 11, 8, 468, 24\n",
    "}\n",
    "\n",
    "# Step 1: Get image IDs that have at least one annotation with a target category ID\n",
    "target_image_ids = set()\n",
    "\n",
    "for ann in data[\"annotations\"]:\n",
    "    if ann[\"category_id\"] in target_category_ids:\n",
    "        target_image_ids.add(ann[\"image_id\"])\n",
    "\n",
    "# Step 2: Map image_id -> file name\n",
    "id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in data[\"images\"]}\n",
    "\n",
    "# Step 3: Create list of full GCS paths\n",
    "output_paths = []\n",
    "for img_id in target_image_ids:\n",
    "    if img_id in id_to_filename:\n",
    "        file_path = id_to_filename[img_id]\n",
    "        gcs_path = f\"gs://public-datasets-lila/wcs-unzipped/animals/{file_path}\"\n",
    "        output_paths.append(gcs_path)\n",
    "\n",
    "# Step 4: Save to a file\n",
    "with open(\"target_animal_image_paths.txt\", \"w\") as f:\n",
    "    for path in output_paths:\n",
    "        f.write(path + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(output_paths)} image paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac433aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "INPUT_FILE = \"target_animal_image_paths.txt\"\n",
    "OUTPUT_DIR = \"downloaded_images\"\n",
    "MAX_WORKERS = 10  # Threads, adjust depending on bandwidth/CPU\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def download_image(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    rel_path = line.replace(\"gs://public-datasets-lila/wcs-unzipped/\", \"\")\n",
    "    safe_name = rel_path.replace(\"/\", \"_\")\n",
    "    output_path = os.path.join(OUTPUT_DIR, safe_name)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        return f\"‚úî Skipped: {safe_name}\"\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"gsutil\", \"cp\", line, output_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            return f\"‚úÖ Downloaded: {safe_name}\"\n",
    "        else:\n",
    "            return f\"‚ùå Failed: {safe_name} ‚Äî {result.stderr.decode().strip()}\"\n",
    "    except Exception as e:\n",
    "        return f\"üí• Error: {safe_name} ‚Äî {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    with open(INPUT_FILE, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(download_image, line) for line in lines]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Paths ===\n",
    "coco_json_path = \"wcs_camera_traps.json\"          \n",
    "download_dir = \"downloaded_images\"           \n",
    "output_dir = \"organized_by_species\"          \n",
    "\n",
    "# === Load JSON ===\n",
    "with open(coco_json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# === Map categories and images ===\n",
    "category_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
    "image_id_to_file = {\n",
    "    img[\"id\"]: img[\"file_name\"].replace(\"/\", \"_\")\n",
    "    for img in data[\"images\"]\n",
    "}\n",
    "\n",
    "# === Map file_name to species ===\n",
    "file_to_species = defaultdict(list)\n",
    "\n",
    "for ann in data[\"annotations\"]:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    cat_id = ann[\"category_id\"]\n",
    "    file_name = image_id_to_file.get(img_id)\n",
    "    species = category_id_to_name.get(cat_id)\n",
    "    if file_name and species:\n",
    "        file_to_species[file_name].append(species)\n",
    "\n",
    "# === Organize files ===\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file_name, species_list in file_to_species.items():\n",
    "    src_path = os.path.join(download_dir, file_name)\n",
    "    if not os.path.exists(src_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping {file_name} (not found)\")\n",
    "        continue\n",
    "\n",
    "    for species in set(species_list):  # avoid duplicates\n",
    "        species_dir = os.path.join(output_dir, species)\n",
    "        os.makedirs(species_dir, exist_ok=True)\n",
    "        dst_path = os.path.join(species_dir, file_name)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        print(f\"üì¶ Copied {file_name} to {species_dir}\")\n",
    "\n",
    "print(\"‚úÖ Done organizing by species!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e62452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "metadata_csv_path = 'na.csv'\n",
    "output_txt_path = 'files_to_download.txt'\n",
    "base_path_in_bucket = 'nacti-unzipped/'\n",
    "\n",
    "# Read the metadata CSV\n",
    "print(f\"Reading metadata from {metadata_csv_path}...\")\n",
    "metadata_df = pd.read_csv(metadata_csv_path)\n",
    "print(f\"Metadata loaded: {len(metadata_df)} rows.\")\n",
    "\n",
    "# Initialize tracking\n",
    "species_download_count = {species: 0 for species in metadata_df['common_name'].unique()}\n",
    "max_images_per_species = 5000\n",
    "\n",
    "# Prepare output file\n",
    "count_total = 0\n",
    "with open(output_txt_path, 'w') as f:\n",
    "    # tqdm progress bar\n",
    "    for index, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Processing images\"):\n",
    "        common_name = row['common_name']\n",
    "        filename = row['filename']\n",
    "\n",
    "        if species_download_count[common_name] < max_images_per_species:\n",
    "            full_path = 'gs://public-datasets-lila/' + base_path_in_bucket + filename\n",
    "            f.write(full_path + '\\n')\n",
    "            species_download_count[common_name] += 1\n",
    "            count_total += 1\n",
    "\n",
    "        # Stop early if we already have enough images\n",
    "        if all(count >= max_images_per_species for count in species_download_count.values()):\n",
    "            print(\"Reached target of 5000 images per species. Stopping early.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nFinished. Total images selected: {count_total}\")\n",
    "print(f\"File list saved to {output_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil -m cp -I < files_to_download.txt lila_species_subset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8091c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "metadata_csv_path = 'na.csv'\n",
    "downloaded_images_path = 'lila_species_subset'\n",
    "organized_dataset_path = 'lila_species_organized'\n",
    "\n",
    "# Load metadata\n",
    "print(f\"Loading metadata from {metadata_csv_path}...\")\n",
    "metadata_df = pd.read_csv(metadata_csv_path)\n",
    "print(f\"Loaded {len(metadata_df)} metadata entries.\")\n",
    "\n",
    "# Create a mapping from just the filename (no path) to species\n",
    "filename_to_species = {os.path.basename(path): species for path, species in zip(metadata_df['filename'], metadata_df['common_name'])}\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(organized_dataset_path, exist_ok=True)\n",
    "\n",
    "# List all files to organize\n",
    "all_files = os.listdir(downloaded_images_path)\n",
    "print(f\"Found {len(all_files)} downloaded images to organize.\")\n",
    "\n",
    "# Organize the files with tqdm progress bar\n",
    "for filename in tqdm(all_files, desc=\"Organizing images\"):\n",
    "    species_name = filename_to_species.get(filename)\n",
    "    if species_name:\n",
    "        # Create a species folder if it doesn't exist\n",
    "        species_folder = os.path.join(organized_dataset_path, species_name)\n",
    "        os.makedirs(species_folder, exist_ok=True)\n",
    "        \n",
    "        # Move the file\n",
    "        src = os.path.join(downloaded_images_path, filename)\n",
    "        dst = os.path.join(species_folder, filename)\n",
    "        shutil.move(src, dst)\n",
    "    else:\n",
    "        print(f\"Warning: {filename} not found in metadata.\")\n",
    "\n",
    "print(\"\\n‚úÖ Done organizing files by species!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce660df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    # Load annotation data\n",
    "    with open(\"wcs_camera_traps.json\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Define the top 50 species (excluding empty/human/unknown)\n",
    "    top_50_species = [\n",
    "        \"tayassu pecari\", \"meleagris ocellata\", \"bos taurus\", \"aepyceros melampus\",\n",
    "        \"equus quagga\", \"crax rubra\", \"dasyprocta punctata\", \"madoqua guentheri\",\n",
    "        \"leopardus pardalis\", \"cephalophus nigrifrons\", \"loxodonta africana\",\n",
    "        \"mitu tuberosum\", \"pecari tajacu\", \"didelphis pernigra\", \"panthera onca\",\n",
    "        \"giraffa camelopardalis\", \"psophia leucoptera\", \"tapirus terrestris\",\n",
    "        \"mazama americana\", \"puma concolor\", \"cuniculus paca\", \"urocyon cinereoargenteus\",\n",
    "        \"syncerus caffer\", \"dasyprocta leporina\", \"mazama temama\", \"muntiacus muntjak\",\n",
    "        \"sylvilagus brasiliensis\", \"coua serriana\", \"capra aegagrus\", \"tapirus bairdii\",\n",
    "        \"papio anubis\", \"macaca nemestrina\", \"cricetomys gambianus\", \"didelphis sp\",\n",
    "        \"tragelaphus oryx\", \"agouti paca\", \"cercopithecus lhoesti\", \"equus grevyi\",\n",
    "        \"penelope jacquacu\", \"nanger granti\", \"crocuta crocuta\", \"equus ferus\",\n",
    "        \"eira barbara\", \"dasypus novemcinctus\", \"argusianus argus\", \"alectoris rufa\",\n",
    "        \"psophia crepitans\", \"francolinus nobilis\", \"didelphis marsupialis\", \"nasua narica\"\n",
    "    ]\n",
    "\n",
    "    # Step 1: Map species names to category_ids\n",
    "    species_to_category_id = {}\n",
    "    category_id_to_species = {}\n",
    "    for category in tqdm(data[\"categories\"], desc=\"Processing categories\"):\n",
    "        if category[\"name\"] in top_50_species:\n",
    "            species_to_category_id[category[\"name\"]] = category[\"id\"]\n",
    "            category_id_to_species[category[\"id\"]] = category[\"name\"].replace(\" \", \"_\")  # Format for folder names\n",
    "\n",
    "    # Step 2: Get image IDs and their species\n",
    "    image_id_to_species = {}\n",
    "    for ann in tqdm(data[\"annotations\"], desc=\"Processing annotations\"):\n",
    "        if ann[\"category_id\"] in category_id_to_species:\n",
    "            image_id_to_species[ann[\"image_id\"]] = category_id_to_species[ann[\"category_id\"]]\n",
    "\n",
    "    # Step 3: Map image_id -> file_name\n",
    "    id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in tqdm(data[\"images\"], desc=\"Processing images\")}\n",
    "\n",
    "    # Step 4: Generate download commands (organized by species)\n",
    "    download_commands = []\n",
    "    for img_id, species_folder in tqdm(image_id_to_species.items(), desc=\"Generating commands\"):\n",
    "        if img_id in id_to_filename:\n",
    "            file_path = id_to_filename[img_id]\n",
    "            gcs_path = f\"gs://public-datasets-lila/wcs-unzipped/{file_path}\"\n",
    "            local_path = os.path.join(species_folder, file_path.split(\"/\")[-1])  # Keep only filename\n",
    "            download_commands.append(f\"{gcs_path} {local_path}\")\n",
    "\n",
    "    # Step 5: Save commands to a file\n",
    "    with open(\"download_commands.sh\", \"w\") as f:\n",
    "        f.write(\"#!/bin/bash\\n\")\n",
    "        \n",
    "        # Create all species folders\n",
    "        species_folders = set(image_id_to_species.values())\n",
    "        for folder in species_folders:\n",
    "            f.write(f\"mkdir -p {folder}\\n\")\n",
    "        \n",
    "        # Batch all downloads into one gsutil -m cp command\n",
    "        f.write(\"gsutil -m cp \\\\\\n\")\n",
    "        for img_id, species_folder in tqdm(image_id_to_species.items(), desc=\"Generating commands\"):\n",
    "            if img_id in id_to_filename:\n",
    "                file_path = id_to_filename[img_id]\n",
    "                gcs_path = f\"gs://public-datasets-lila/wcs-unzipped/{file_path}\"\n",
    "                local_path = os.path.join(species_folder, os.path.basename(file_path))\n",
    "                f.write(f\"  {gcs_path} {local_path} \\\\\\n\")\n",
    "        \n",
    "        f.write(\"  .\\n\")  # Dummy line to close the command\n",
    "\n",
    "    print(f\"‚úÖ Saved {len(download_commands)} download commands to 'download_commands.sh'.\")\n",
    "    print(\"Run this script with: bash download_commands.sh\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657bb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Configuration\n",
    "METADATA_FILE = \"wcs_camera_traps.json/wcs_camera_traps.json\"\n",
    "DOWNLOAD_BASE = \"https://storage.googleapis.com/public-datasets-lila/wcs-unzipped/\"\n",
    "NUM_WORKERS = 8\n",
    "TOP_N_SPECIES = 50\n",
    "MAX_IMAGES_PER_SPECIES = 20000\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def load_metadata():\n",
    "    with open(METADATA_FILE, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading metadata...\")\n",
    "    data = load_metadata()\n",
    "\n",
    "    print(\"Building species index...\")\n",
    "    species_images = defaultdict(list)\n",
    "    category_map = {c[\"id\"]: c[\"name\"] for c in data[\"categories\"]}\n",
    "    image_id_to_file = {img[\"id\"]: img[\"file_name\"] for img in data[\"images\"]}\n",
    "\n",
    "    for ann in tqdm(data[\"annotations\"], desc=\"Indexing\"):\n",
    "        species_name = category_map[ann[\"category_id\"]]\n",
    "        image_id = ann[\"image_id\"]\n",
    "        species_images[species_name].append(image_id)\n",
    "\n",
    "    print(\"Selecting top species...\")\n",
    "    top_species = sorted(\n",
    "        [(k, len(v)) for k, v in species_images.items()],\n",
    "        key=lambda x: -x[1]\n",
    "    )[:TOP_N_SPECIES]\n",
    "\n",
    "    download_queue = []\n",
    "    species_counts = defaultdict(int)\n",
    "\n",
    "    for species, count in tqdm(top_species, desc=\"Sampling\"):\n",
    "        folder_name = species.replace(\" \", \"_\").lower()\n",
    "        base_species_path = os.path.join(\"wcs_download\", folder_name)\n",
    "        os.makedirs(base_species_path, exist_ok=True)\n",
    "\n",
    "        image_ids = random.sample(species_images[species], min(MAX_IMAGES_PER_SPECIES, count))\n",
    "        for img_id in image_ids:\n",
    "            if img_id in image_id_to_file:\n",
    "                img_file = image_id_to_file[img_id]\n",
    "                url = f\"{DOWNLOAD_BASE}{img_file}\"\n",
    "                sub_path = os.path.normpath(img_file)\n",
    "                dest_path = os.path.join(base_species_path, sub_path)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                download_queue.append((url, dest_path, folder_name))\n",
    "                species_counts[folder_name] += 1\n",
    "\n",
    "    def download_file(args):\n",
    "        url, dest_path, folder_name = args\n",
    "        if os.path.exists(dest_path):\n",
    "            return True\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                with open(dest_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    print(f\"Failed to download {url}: {e}\")\n",
    "                    return False\n",
    "\n",
    "    print(f\"\\nDownloading {len(download_queue)} images...\")\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(download_file, download_queue),\n",
    "            total=len(download_queue),\n",
    "            desc=\"Progress\"\n",
    "        ))\n",
    "\n",
    "    print(\"\\nDownload Summary:\")\n",
    "    for species, count in species_counts.items():\n",
    "        species_path = os.path.join(\"wcs_download\", species)\n",
    "        total_files = sum([len(files) for _, _, files in os.walk(species_path)])\n",
    "        print(f\"{species:20s}: {total_files:5d}/{min(MAX_IMAGES_PER_SPECIES, count):5d}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

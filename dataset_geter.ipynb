{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil -m cp -r \"gs://public-datasets-lila/wcs-unzipped/*\" ./wcs_dataset_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 - <<EOF\n",
    "import ijson\n",
    "from collections import defaultdict\n",
    "\n",
    "# Count category occurrences from local file\n",
    "category_counts = defaultdict(int)\n",
    "with open(\"wcs_camera_traps.json\", \"rb\") as f:  # 'rb' mode for ijson\n",
    "    for ann in ijson.items(f, \"annotations.item\"):\n",
    "        category_counts[ann[\"category_id\"]] += 1\n",
    "        print(f\"{ann['category_name']}:{category_counts[ann['category_id']]}\")\n",
    "\n",
    "# Get top 20 categories\n",
    "top_20 = sorted(category_counts.items(), key=lambda x: -x[1])[:20]\n",
    "top_20_ids = [cat_id for cat_id, _ in top_20]\n",
    "\n",
    "print(\"Top 20 category IDs:\", top_20_ids)\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load annotation data\n",
    "with open(\"wcs_camera_traps.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define target category IDs\n",
    "target_category_ids = {\n",
    "    2, 372, 71, 96, 111, 374, 3, 115, 10,\n",
    "    317, 90, 11, 8, 468, 24\n",
    "}\n",
    "\n",
    "# Step 1: Get image IDs that have at least one annotation with a target category ID\n",
    "target_image_ids = set()\n",
    "\n",
    "for ann in data[\"annotations\"]:\n",
    "    if ann[\"category_id\"] in target_category_ids:\n",
    "        target_image_ids.add(ann[\"image_id\"])\n",
    "\n",
    "# Step 2: Map image_id -> file name\n",
    "id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in data[\"images\"]}\n",
    "\n",
    "# Step 3: Create list of full GCS paths\n",
    "output_paths = []\n",
    "for img_id in target_image_ids:\n",
    "    if img_id in id_to_filename:\n",
    "        file_path = id_to_filename[img_id]\n",
    "        gcs_path = f\"gs://public-datasets-lila/wcs-unzipped/animals/{file_path}\"\n",
    "        output_paths.append(gcs_path)\n",
    "\n",
    "# Step 4: Save to a file\n",
    "with open(\"target_animal_image_paths.txt\", \"w\") as f:\n",
    "    for path in output_paths:\n",
    "        f.write(path + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(output_paths)} image paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac433aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "INPUT_FILE = \"target_animal_image_paths.txt\"\n",
    "OUTPUT_DIR = \"downloaded_images\"\n",
    "MAX_WORKERS = 10  # Threads, adjust depending on bandwidth/CPU\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def download_image(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    rel_path = line.replace(\"gs://public-datasets-lila/wcs-unzipped/\", \"\")\n",
    "    safe_name = rel_path.replace(\"/\", \"_\")\n",
    "    output_path = os.path.join(OUTPUT_DIR, safe_name)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        return f\"‚úî Skipped: {safe_name}\"\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"gsutil\", \"cp\", line, output_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            return f\"‚úÖ Downloaded: {safe_name}\"\n",
    "        else:\n",
    "            return f\"‚ùå Failed: {safe_name} ‚Äî {result.stderr.decode().strip()}\"\n",
    "    except Exception as e:\n",
    "        return f\"üí• Error: {safe_name} ‚Äî {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    with open(INPUT_FILE, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(download_image, line) for line in lines]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Paths ===\n",
    "coco_json_path = \"wcs_camera_traps.json\"          \n",
    "download_dir = \"downloaded_images\"           \n",
    "output_dir = \"organized_by_species\"          \n",
    "\n",
    "# === Load JSON ===\n",
    "with open(coco_json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# === Map categories and images ===\n",
    "category_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
    "image_id_to_file = {\n",
    "    img[\"id\"]: img[\"file_name\"].replace(\"/\", \"_\")\n",
    "    for img in data[\"images\"]\n",
    "}\n",
    "\n",
    "# === Map file_name to species ===\n",
    "file_to_species = defaultdict(list)\n",
    "\n",
    "for ann in data[\"annotations\"]:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    cat_id = ann[\"category_id\"]\n",
    "    file_name = image_id_to_file.get(img_id)\n",
    "    species = category_id_to_name.get(cat_id)\n",
    "    if file_name and species:\n",
    "        file_to_species[file_name].append(species)\n",
    "\n",
    "# === Organize files ===\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file_name, species_list in file_to_species.items():\n",
    "    src_path = os.path.join(download_dir, file_name)\n",
    "    if not os.path.exists(src_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping {file_name} (not found)\")\n",
    "        continue\n",
    "\n",
    "    for species in set(species_list):  # avoid duplicates\n",
    "        species_dir = os.path.join(output_dir, species)\n",
    "        os.makedirs(species_dir, exist_ok=True)\n",
    "        dst_path = os.path.join(species_dir, file_name)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        print(f\"üì¶ Copied {file_name} to {species_dir}\")\n",
    "\n",
    "print(\"‚úÖ Done organizing by species!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e62452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "metadata_csv_path = 'na.csv'\n",
    "output_txt_path = 'files_to_download.txt'\n",
    "base_path_in_bucket = 'nacti-unzipped/'\n",
    "\n",
    "# Read the metadata CSV\n",
    "print(f\"Reading metadata from {metadata_csv_path}...\")\n",
    "metadata_df = pd.read_csv(metadata_csv_path)\n",
    "print(f\"Metadata loaded: {len(metadata_df)} rows.\")\n",
    "\n",
    "# Initialize tracking\n",
    "species_download_count = {species: 0 for species in metadata_df['common_name'].unique()}\n",
    "max_images_per_species = 5000\n",
    "\n",
    "# Prepare output file\n",
    "count_total = 0\n",
    "with open(output_txt_path, 'w') as f:\n",
    "    # tqdm progress bar\n",
    "    for index, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Processing images\"):\n",
    "        common_name = row['common_name']\n",
    "        filename = row['filename']\n",
    "\n",
    "        if species_download_count[common_name] < max_images_per_species:\n",
    "            full_path = 'gs://public-datasets-lila/' + base_path_in_bucket + filename\n",
    "            f.write(full_path + '\\n')\n",
    "            species_download_count[common_name] += 1\n",
    "            count_total += 1\n",
    "\n",
    "        # Stop early if we already have enough images\n",
    "        if all(count >= max_images_per_species for count in species_download_count.values()):\n",
    "            print(\"Reached target of 5000 images per species. Stopping early.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nFinished. Total images selected: {count_total}\")\n",
    "print(f\"File list saved to {output_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil -m cp -I < files_to_download.txt lila_species_subset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8091c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "metadata_csv_path = 'na.csv'\n",
    "downloaded_images_path = 'lila_species_subset'\n",
    "organized_dataset_path = 'lila_species_organized'\n",
    "\n",
    "# Load metadata\n",
    "print(f\"Loading metadata from {metadata_csv_path}...\")\n",
    "metadata_df = pd.read_csv(metadata_csv_path)\n",
    "print(f\"Loaded {len(metadata_df)} metadata entries.\")\n",
    "\n",
    "# Create a mapping from just the filename (no path) to species\n",
    "filename_to_species = {os.path.basename(path): species for path, species in zip(metadata_df['filename'], metadata_df['common_name'])}\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(organized_dataset_path, exist_ok=True)\n",
    "\n",
    "# List all files to organize\n",
    "all_files = os.listdir(downloaded_images_path)\n",
    "print(f\"Found {len(all_files)} downloaded images to organize.\")\n",
    "\n",
    "# Organize the files with tqdm progress bar\n",
    "for filename in tqdm(all_files, desc=\"Organizing images\"):\n",
    "    species_name = filename_to_species.get(filename)\n",
    "    if species_name:\n",
    "        # Create a species folder if it doesn't exist\n",
    "        species_folder = os.path.join(organized_dataset_path, species_name)\n",
    "        os.makedirs(species_folder, exist_ok=True)\n",
    "        \n",
    "        # Move the file\n",
    "        src = os.path.join(downloaded_images_path, filename)\n",
    "        dst = os.path.join(species_folder, filename)\n",
    "        shutil.move(src, dst)\n",
    "    else:\n",
    "        print(f\"Warning: {filename} not found in metadata.\")\n",
    "\n",
    "print(\"\\n‚úÖ Done organizing files by species!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

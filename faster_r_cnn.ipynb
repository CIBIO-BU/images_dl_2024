{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time \n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Config\n",
    "data_yaml = 'data.yaml'  # Your dataset config\n",
    "batch_size = 2\n",
    "image_size = 512\n",
    "epochs = 20\n",
    "\n",
    "# Dataset\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, yaml_path, mode='train'):\n",
    "        with open(yaml_path) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        self.img_dir = os.path.join(data['path'], data[mode])\n",
    "        self.label_dir = self.img_dir.replace('images', 'labels')\n",
    "        self.images = [f for f in os.listdir(self.img_dir) \n",
    "                      if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.classes = data['names']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = torchvision.transforms.functional.to_tensor(img)\n",
    "        img = torchvision.transforms.functional.resize(img, [image_size]*2)\n",
    "        \n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, \n",
    "                                os.path.splitext(self.images[idx])[0] + '.txt')\n",
    "        boxes, labels = [], []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    class_id, xc, yc, w, h = map(float, line.strip().split())\n",
    "                    # Convert YOLO to Pascal VOC\n",
    "                    x1 = (xc - w/2) * image_size\n",
    "                    y1 = (yc - h/2) * image_size\n",
    "                    x2 = (xc + w/2) * image_size\n",
    "                    y2 = (yc + h/2) * image_size\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(class_id) + 1)  # +1 because background is class 0\n",
    "        \n",
    "        target = {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (torch.tensor(boxes)[:, 3] - torch.tensor(boxes)[:, 1]) * \n",
    "                    (torch.tensor(boxes)[:, 2] - torch.tensor(boxes)[:, 0]),\n",
    "            'iscrowd': torch.zeros(len(labels), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "# Model\n",
    "def create_model(num_classes):\n",
    "    backbone = torchvision.models.mobilenet_v2(weights='DEFAULT').features\n",
    "    backbone.out_channels = 1280  # MobilenetV2 feature dimension\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    \n",
    "    return FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "\n",
    "# Training\n",
    "def train():\n",
    "    # Data\n",
    "    train_set = YOLODataset(data_yaml, 'train')\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = create_model(len(train_set.classes) + 1).to('cuda')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting training on {len(train_set)} images\")\n",
    "    print(f\"ðŸ“¦ Batch size: {batch_size} | ðŸ”„ Total batches: {len(train_loader)}\")\n",
    "    print(f\"ðŸ”¥ Epochs: {epochs} | ðŸ’» Device: {next(model.parameters()).device}\\n\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            batch_start = time.time()\n",
    "\n",
    "            # Move to GPU\n",
    "            images = [img.to('cuda') for img in images]\n",
    "            targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward + backward\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_time = time.time() - batch_start\n",
    "            epoch_loss += losses.item()\n",
    "            avg_loss = epoch_loss / (batch_idx + 1)\n",
    "\n",
    "            print(\n",
    "                f\"\\rEpoch {epoch+1}/{epochs} | \"\n",
    "                f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                f\"Loss: {losses.item():.3f} (avg: {avg_loss:.3f}) | \"\n",
    "                f\"Time: {batch_time:.2f}s/batch | \"\n",
    "                f\"Mem: {torch.cuda.memory_allocated()/1e9:.2f}GB\",\n",
    "                end=\"\", flush=True\n",
    "            )                    \n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\nâœ… Epoch {epoch+1} complete | \"\n",
    "              f\"Avg loss: {epoch_loss/len(train_loader):.4f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.ops import box_iou\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"weights_path\": \"fasterrcnn_epoch20.pth\",\n",
    "    \"data_yaml\": \"data.yaml\",\n",
    "    \"batch_size\": 4,\n",
    "    \"img_size\": 512,\n",
    "    \"conf_thresh\": 0.5,\n",
    "    \"iou_thresh\": 0.5,\n",
    "    \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \"output_dir\": \"evaluation_results\"\n",
    "}\n",
    "\n",
    "# Setup output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, yaml_path, mode='val'):\n",
    "        with open(yaml_path) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        self.img_dir = os.path.join(data['path'], data[mode])\n",
    "        self.label_dir = self.img_dir.replace('images', 'labels')\n",
    "        self.images = sorted([f for f in os.listdir(self.img_dir) \n",
    "                           if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        self.class_names = data['names']\n",
    "        self.class_dict = {i: name for i, name in enumerate(self.class_names)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = torchvision.transforms.functional.to_tensor(img)\n",
    "        img = torchvision.transforms.functional.resize(img, [CONFIG['img_size']]*2)\n",
    "        \n",
    "        label_path = os.path.join(self.label_dir, \n",
    "                                os.path.splitext(self.images[idx])[0] + '.txt')\n",
    "        boxes, labels = [], []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 5:\n",
    "                        continue\n",
    "                    class_id, xc, yc, w, h = map(float, parts[:5])\n",
    "                    x1 = (xc - w/2) * CONFIG['img_size']\n",
    "                    y1 = (yc - h/2) * CONFIG['img_size']\n",
    "                    x2 = (xc + w/2) * CONFIG['img_size']\n",
    "                    y2 = (yc + h/2) * CONFIG['img_size']\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(class_id))\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"image_name\": self.images[idx]\n",
    "        }\n",
    "        return img, target\n",
    "\n",
    "def load_model(num_classes):\n",
    "    backbone = torchvision.models.mobilenet_v2(weights=None).features\n",
    "    backbone.out_channels = 1280\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=679,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "    state_dict = torch.load(CONFIG['weights_path'], map_location=CONFIG['device'],weights_only=True)\n",
    "    for key in list(state_dict.keys()):\n",
    "        if 'cls_score' in key or 'bbox_pred' in key:\n",
    "            del state_dict[key]\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    return model.to(CONFIG['device'])\n",
    "\n",
    "def calculate_metrics(pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels):\n",
    "    \"\"\"Calculate precision, recall, and AP for each class\"\"\"\n",
    "    metrics = {\n",
    "        'true_positives': 0,\n",
    "        'false_positives': 0,\n",
    "        'false_negatives': 0,\n",
    "        'precision': 0.0,\n",
    "        'recall': 0.0,\n",
    "        'f1': 0.0,\n",
    "        'ious': []\n",
    "    }\n",
    "    \n",
    "    if len(gt_boxes) == 0:\n",
    "        return metrics\n",
    "    \n",
    "    # Filter predictions by confidence\n",
    "    keep = pred_scores >= CONFIG['conf_thresh']\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    pred_labels = pred_labels[keep]\n",
    "    \n",
    "    # Calculate IoU matrix\n",
    "    iou_matrix = box_iou(gt_boxes, pred_boxes).cpu().numpy()\n",
    "    \n",
    "    # Match predictions to ground truth\n",
    "    matches = []\n",
    "    for gt_idx in range(len(gt_boxes)):\n",
    "        for pred_idx in range(len(pred_boxes)):\n",
    "            if (iou_matrix[gt_idx, pred_idx] >= CONFIG['iou_thresh'] and \n",
    "                gt_labels[gt_idx] == pred_labels[pred_idx]):\n",
    "                matches.append((gt_idx, pred_idx, iou_matrix[gt_idx, pred_idx]))\n",
    "    \n",
    "    # Sort matches by IoU\n",
    "    matches.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Count TP, FP, FN\n",
    "    matched_gt = set()\n",
    "    matched_pred = set()\n",
    "    for gt_idx, pred_idx, iou in matches:\n",
    "        if gt_idx not in matched_gt and pred_idx not in matched_pred:\n",
    "            metrics['true_positives'] += 1\n",
    "            metrics['ious'].append(iou)\n",
    "            matched_gt.add(gt_idx)\n",
    "            matched_pred.add(pred_idx)\n",
    "    \n",
    "    metrics['false_positives'] = len(pred_boxes) - len(matched_pred)\n",
    "    metrics['false_negatives'] = len(gt_boxes) - len(matched_gt)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics['precision'] = metrics['true_positives'] / max(metrics['true_positives'] + metrics['false_positives'], 1)\n",
    "    metrics['recall'] = metrics['true_positives'] / max(metrics['true_positives'] + metrics['false_negatives'], 1)\n",
    "    metrics['f1'] = 2 * (metrics['precision'] * metrics['recall']) / max(metrics['precision'] + metrics['recall'], 1e-6)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_detections(image, gt_boxes, gt_labels, pred_boxes, pred_labels, pred_scores, class_names, save_path):\n",
    "    \"\"\"Visualize ground truth and predictions on an image\"\"\"\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image.permute(1, 2, 0))\n",
    "    \n",
    "    # Draw ground truth (green)\n",
    "    for box, label in zip(gt_boxes, gt_labels):\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
    "            linewidth=2, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(box[0], box[1], class_names[label], color='white',\n",
    "                bbox=dict(facecolor='green', alpha=0.7, pad=1))\n",
    "    \n",
    "    # Draw predictions (red)\n",
    "    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
    "            linewidth=2, edgecolor='r', facecolor='none', linestyle='--')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(box[0], box[1], f\"{class_names[label]} {score:.2f}\", color='white',\n",
    "                bbox=dict(facecolor='red', alpha=0.7, pad=1))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def evaluate():\n",
    "    # Load dataset and model\n",
    "    dataset = EvaluationDataset(CONFIG['data_yaml'], 'val')\n",
    "    dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'], \n",
    "                          collate_fn=lambda x: tuple(zip(*x)))\n",
    "    model = load_model(679)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    class_metrics = defaultdict(lambda: {\n",
    "        'true_positives': 0,\n",
    "        'false_positives': 0,\n",
    "        'false_negatives': 0,\n",
    "        'ious': []\n",
    "    })\n",
    "    image_results = []\n",
    "    \n",
    "    # Evaluation loop\n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "        images = [img.to(CONFIG['device']) for img in images]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "        \n",
    "        for i, (target, output) in enumerate(zip(targets, outputs)):\n",
    "            # Move data to CPU for metrics calculation\n",
    "            gt_boxes = target['boxes'].cpu()\n",
    "            gt_labels = target['labels'].cpu()\n",
    "            pred_boxes = output['boxes'].cpu()\n",
    "            pred_labels = output['labels'].cpu()\n",
    "            pred_scores = output['scores'].cpu()\n",
    "            \n",
    "            # Calculate metrics per class\n",
    "            for class_id in range(len(dataset.class_names)):\n",
    "                class_mask = gt_labels == class_id\n",
    "                class_gt_boxes = gt_boxes[class_mask]\n",
    "                class_gt_labels = gt_labels[class_mask]\n",
    "                \n",
    "                pred_mask = pred_labels == class_id\n",
    "                class_pred_boxes = pred_boxes[pred_mask]\n",
    "                class_pred_labels = pred_labels[pred_mask]\n",
    "                class_pred_scores = pred_scores[pred_mask]\n",
    "                \n",
    "                metrics = calculate_metrics(\n",
    "                    class_pred_boxes, class_pred_labels, class_pred_scores,\n",
    "                    class_gt_boxes, class_gt_labels\n",
    "                )\n",
    "                \n",
    "                # Update class metrics\n",
    "                class_metrics[class_id]['true_positives'] += metrics['true_positives']\n",
    "                class_metrics[class_id]['false_positives'] += metrics['false_positives']\n",
    "                class_metrics[class_id]['false_negatives'] += metrics['false_negatives']\n",
    "                class_metrics[class_id]['ious'].extend(metrics['ious'])\n",
    "            \n",
    "            # Save visualization for first image in first batch\n",
    "            if batch_idx == 0 and i == 0:\n",
    "                vis_path = os.path.join(CONFIG['output_dir'], 'detection_example.png')\n",
    "                visualize_detections(\n",
    "                    images[i].cpu(), gt_boxes, gt_labels,\n",
    "                    pred_boxes, pred_labels, pred_scores,\n",
    "                    dataset.class_names, vis_path\n",
    "                )\n",
    "            \n",
    "            # Store per-image results\n",
    "            image_results.append({\n",
    "                'image_name': target['image_name'],\n",
    "                'ground_truth': {\n",
    "                    'boxes': gt_boxes.tolist(),\n",
    "                    'labels': gt_labels.tolist()\n",
    "                },\n",
    "                'predictions': {\n",
    "                    'boxes': pred_boxes.tolist(),\n",
    "                    'labels': pred_labels.tolist(),\n",
    "                    'scores': pred_scores.tolist()\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    results = {\n",
    "        'per_class': {},\n",
    "        'overall': {\n",
    "            'true_positives': 0,\n",
    "            'false_positives': 0,\n",
    "            'false_negatives': 0,\n",
    "            'mean_iou': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Aggregate class metrics\n",
    "    for class_id, metrics in class_metrics.items():\n",
    "        tp = metrics['true_positives']\n",
    "        fp = metrics['false_positives']\n",
    "        fn = metrics['false_negatives']\n",
    "        ious = metrics['ious']\n",
    "        \n",
    "        precision = tp / max(tp + fp, 1)\n",
    "        recall = tp / max(tp + fn, 1)\n",
    "        f1 = 2 * (precision * recall) / max(precision + recall, 1e-6)\n",
    "        mean_iou = np.mean(ious) if ious else 0.0\n",
    "        \n",
    "        results['per_class'][dataset.class_names[class_id]] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'mean_iou': mean_iou\n",
    "        }\n",
    "        \n",
    "        # Update overall metrics\n",
    "        results['overall']['true_positives'] += tp\n",
    "        results['overall']['false_positives'] += fp\n",
    "        results['overall']['false_negatives'] += fn\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    tp = results['overall']['true_positives']\n",
    "    fp = results['overall']['false_positives']\n",
    "    fn = results['overall']['false_negatives']\n",
    "    \n",
    "    results['overall']['precision'] = tp / max(tp + fp, 1)\n",
    "    results['overall']['recall'] = tp / max(tp + fn, 1)\n",
    "    results['overall']['f1'] = 2 * (results['overall']['precision'] * results['overall']['recall']) / \\\n",
    "                              max(results['overall']['precision'] + results['overall']['recall'], 1e-6)\n",
    "    \n",
    "    # Save results\n",
    "    with open(os.path.join(CONFIG['output_dir'], 'metrics.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    with open(os.path.join(CONFIG['output_dir'], 'per_image_results.json'), 'w') as f:\n",
    "        json.dump(image_results, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"{'Class':<15} {'Precision':>10} {'Recall':>10} {'F1':>10} {'mIoU':>10}\")\n",
    "    print(\"-\" * 55)\n",
    "    for class_name, metrics in results['per_class'].items():\n",
    "        print(f\"{class_name:<15} {metrics['precision']:>10.4f} {metrics['recall']:>10.4f} \"\n",
    "              f\"{metrics['f1']:>10.4f} {metrics['mean_iou']:>10.4f}\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'OVERALL':<15} {results['overall']['precision']:>10.4f} \"\n",
    "          f\"{results['overall']['recall']:>10.4f} {results['overall']['f1']:>10.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

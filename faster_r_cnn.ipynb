{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time \n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Config\n",
    "data_yaml = 'data.yaml'  # Your dataset config\n",
    "batch_size = 2\n",
    "image_size = 512\n",
    "epochs = 20\n",
    "\n",
    "# Dataset\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, yaml_path, mode='train'):\n",
    "        with open(yaml_path) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        self.img_dir = os.path.join(data['path'], data[mode])\n",
    "        self.label_dir = self.img_dir.replace('images', 'labels')\n",
    "        self.images = [f for f in os.listdir(self.img_dir) \n",
    "                      if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.classes = data['names']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = torchvision.transforms.functional.to_tensor(img)\n",
    "        img = torchvision.transforms.functional.resize(img, [image_size]*2)\n",
    "        \n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, \n",
    "                                os.path.splitext(self.images[idx])[0] + '.txt')\n",
    "        boxes, labels = [], []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    class_id, xc, yc, w, h = map(float, line.strip().split())\n",
    "                    # Convert YOLO to Pascal VOC\n",
    "                    x1 = (xc - w/2) * image_size\n",
    "                    y1 = (yc - h/2) * image_size\n",
    "                    x2 = (xc + w/2) * image_size\n",
    "                    y2 = (yc + h/2) * image_size\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(class_id) + 1)  # +1 because background is class 0\n",
    "        \n",
    "        target = {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (torch.tensor(boxes)[:, 3] - torch.tensor(boxes)[:, 1]) * \n",
    "                    (torch.tensor(boxes)[:, 2] - torch.tensor(boxes)[:, 0]),\n",
    "            'iscrowd': torch.zeros(len(labels), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "# Model\n",
    "def create_model(num_classes):\n",
    "    backbone = torchvision.models.mobilenet_v2(weights='DEFAULT').features\n",
    "    backbone.out_channels = 1280  # MobilenetV2 feature dimension\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    \n",
    "    return FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "\n",
    "# Training\n",
    "def train():\n",
    "    # Data\n",
    "    train_set = YOLODataset(data_yaml, 'train')\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = create_model(len(train_set.classes) + 1).to('cuda')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting training on {len(train_set)} images\")\n",
    "    print(f\"ðŸ“¦ Batch size: {batch_size} | ðŸ”„ Total batches: {len(train_loader)}\")\n",
    "    print(f\"ðŸ”¥ Epochs: {epochs} | ðŸ’» Device: {next(model.parameters()).device}\\n\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            batch_start = time.time()\n",
    "\n",
    "            # Move to GPU\n",
    "            images = [img.to('cuda') for img in images]\n",
    "            targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward + backward\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_time = time.time() - batch_start\n",
    "            epoch_loss += losses.item()\n",
    "            avg_loss = epoch_loss / (batch_idx + 1)\n",
    "\n",
    "            print(\n",
    "                f\"\\rEpoch {epoch+1}/{epochs} | \"\n",
    "                f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                f\"Loss: {losses.item():.3f} (avg: {avg_loss:.3f}) | \"\n",
    "                f\"Time: {batch_time:.2f}s/batch | \"\n",
    "                f\"Mem: {torch.cuda.memory_allocated()/1e9:.2f}GB\",\n",
    "                end=\"\", flush=True\n",
    "            )                    \n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\nâœ… Epoch {epoch+1} complete | \"\n",
    "              f\"Avg loss: {epoch_loss/len(train_loader):.4f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time \n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config\n",
    "data_yaml = 'data.yaml'  # Your dataset config\n",
    "batch_size = 2\n",
    "image_size = 512\n",
    "epochs = 20\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Dataset\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, yaml_path, mode='train', augment=False):\n",
    "        with open(yaml_path) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        self.img_dir = os.path.join(data['path'], data[mode])\n",
    "        self.label_dir = self.img_dir.replace('images', 'labels')\n",
    "        self.images = [f for f in os.listdir(self.img_dir) \n",
    "                      if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.classes = data['names']\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Define transforms\n",
    "        self.base_transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Resize([image_size]*2)\n",
    "        ])\n",
    "        \n",
    "        if self.augment:\n",
    "            self.transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "                self.base_transform\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = self.base_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, \n",
    "                                os.path.splitext(self.images[idx])[0] + '.txt')\n",
    "        boxes, labels = [], []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:  # Make sure line has correct format\n",
    "                        class_id, xc, yc, w, h = map(float, parts)\n",
    "                        # Convert YOLO to Pascal VOC\n",
    "                        x1 = (xc - w/2) * image_size\n",
    "                        y1 = (yc - h/2) * image_size\n",
    "                        x2 = (xc + w/2) * image_size\n",
    "                        y2 = (yc + h/2) * image_size\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        labels.append(int(class_id) + 1)  # +1 because background is class 0\n",
    "        \n",
    "        # Handle empty boxes case\n",
    "        if len(boxes) == 0:\n",
    "            target = {\n",
    "                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': torch.zeros(0, dtype=torch.float32),\n",
    "                'iscrowd': torch.zeros(0, dtype=torch.int64)\n",
    "            }\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            target = {\n",
    "                'boxes': boxes,\n",
    "                'labels': torch.tensor(labels, dtype=torch.int64),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros(len(labels), dtype=torch.int64)\n",
    "            }\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "# Model\n",
    "def create_model(num_classes):\n",
    "    backbone = torchvision.models.mobilenet_v2(weights='DEFAULT').features\n",
    "    backbone.out_channels = 1280  # MobilenetV2 feature dimension\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    \n",
    "    return FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "\n",
    "# Validation function\n",
    "def validate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Validating\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Temporarily switch to train mode to get losses\n",
    "            model.train()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            model.eval()\n",
    "            \n",
    "            val_loss += losses.item()\n",
    "    \n",
    "    return val_loss / len(data_loader)\n",
    "\n",
    "# Training\n",
    "def train():\n",
    "    # Data\n",
    "    train_set = YOLODataset(data_yaml, 'train', augment=True)\n",
    "    val_set = YOLODataset(data_yaml, 'val')  # Assuming 'val' exists in your data.yaml\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = create_model(len(train_set.classes) + 1).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting training on {len(train_set)} images, validating on {len(val_set)} images\")\n",
    "    print(f\"ðŸ“¦ Batch size: {batch_size} | ðŸ”„ Total batches: {len(train_loader)}\")\n",
    "    print(f\"ðŸ”¥ Epochs: {epochs} | ðŸ’» Device: {device}\\n\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            batch_start = time.time()\n",
    "\n",
    "            # Move to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward + backward with mixed precision\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += losses.item()\n",
    "            \n",
    "            # Print batch stats\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                avg_loss = epoch_loss / (batch_idx + 1)\n",
    "                print(\n",
    "                    f\"\\rEpoch {epoch+1}/{epochs} | \"\n",
    "                    f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                    f\"Loss: {losses.item():.3f} (avg: {avg_loss:.3f}) | \"\n",
    "                    f\"Mem: {torch.cuda.memory_allocated()/1e9:.2f}GB\",\n",
    "                    end=\"\", flush=True\n",
    "                )\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = validate(model, val_loader, device)\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"\\nâœ… Epoch {epoch+1} complete | \"\n",
    "              f\"Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "            print(f\"ðŸ’¾ Saved best model with val loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Periodic checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch+1}.pth')\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"ðŸ’¾ Saved checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "import yaml\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "# CONFIGURATION\n",
    "MODEL_PATH = \"checkpoints/best_model.pth\"\n",
    "DATA_YAML = \"data.yaml\"\n",
    "IMAGE_DIR = \"images/val\"\n",
    "LABEL_DIR = \"labels/val\"\n",
    "IMAGE_SIZE = 512\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# Load class names from data.yaml\n",
    "with open(DATA_YAML) as f:\n",
    "    data = yaml.safe_load(f)\n",
    "CLASS_NAMES = data['names']\n",
    "NUM_CLASSES = len(CLASS_NAMES) + 1  # +1 for background\n",
    "\n",
    "def create_model():\n",
    "    backbone = torchvision.models.mobilenet_v2(weights=None).features\n",
    "    backbone.out_channels = 1280\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    return FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "\n",
    "def load_data():\n",
    "    image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    ])\n",
    "\n",
    "    data = []\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(IMAGE_DIR, img_file)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = transform(img)\n",
    "\n",
    "        label_path = os.path.join(LABEL_DIR, os.path.splitext(img_file)[0] + '.txt')\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:\n",
    "                        class_id, xc, yc, w, h = map(float, parts)\n",
    "                        x1 = (xc - w/2) * IMAGE_SIZE\n",
    "                        y1 = (yc - h/2) * IMAGE_SIZE\n",
    "                        x2 = (xc + w/2) * IMAGE_SIZE\n",
    "                        y2 = (yc + h/2) * IMAGE_SIZE\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        labels.append(int(class_id) + 1)\n",
    "\n",
    "        target = {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "        }\n",
    "        data.append((img_tensor, target))\n",
    "    return data\n",
    "\n",
    "def evaluate_model():\n",
    "    model = create_model().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "\n",
    "    data = load_data()\n",
    "    metric = MeanAveragePrecision(\n",
    "        box_format='xyxy',\n",
    "        iou_thresholds=[0.5],\n",
    "        class_metrics=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, target in tqdm(data, desc=\"Evaluating\"):\n",
    "            img = img.unsqueeze(0).to(DEVICE)\n",
    "            prediction = model(img)[0]\n",
    "\n",
    "            keep = prediction['scores'] > CONFIDENCE_THRESHOLD\n",
    "            filtered_pred = {\n",
    "                'boxes': prediction['boxes'][keep],\n",
    "                'scores': prediction['scores'][keep],\n",
    "                'labels': prediction['labels'][keep]\n",
    "            }\n",
    "\n",
    "            # Move target to same device\n",
    "            target = {\n",
    "                'boxes': target['boxes'].to(DEVICE),\n",
    "                'labels': target['labels'].to(DEVICE)\n",
    "            }\n",
    "\n",
    "            metric.update([filtered_pred], [target])\n",
    "\n",
    "\n",
    "    # Handle different torchmetrics versions\n",
    "    results = metric.compute()\n",
    "    metrics = {\n",
    "        'map_50': results.get('map_50', results.get('map@0.5', torch.tensor(0.))).item(),\n",
    "        'precision': results.get('map_per_class', torch.tensor([0.]*NUM_CLASSES)).mean().item(),\n",
    "        'recall': results.get('mar_50', results.get('recall@0.5', torch.tensor(0.))).item(),\n",
    "        'per_class': results.get('map_per_class', torch.tensor([0.]*NUM_CLASSES)).tolist()\n",
    "    }\n",
    "\n",
    "    print(\"\\nðŸ“Š Evaluation Results:\")\n",
    "    print(f\"mAP@0.5: {metrics['map_50']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-Class AP@0.5:\")\n",
    "    for i, class_name in enumerate(CLASS_NAMES):\n",
    "        print(f\"{class_name}: {metrics['per_class'][i]:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
